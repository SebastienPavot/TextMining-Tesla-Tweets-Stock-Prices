{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Parameters-Definition:\" data-toc-modified-id=\"Parameters-Definition:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Parameters Definition:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tweepy-connection:\" data-toc-modified-id=\"Tweepy-connection:-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Tweepy connection:</a></span></li><li><span><a href=\"#Search-parameters:\" data-toc-modified-id=\"Search-parameters:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Search parameters:</a></span></li><li><span><a href=\"#SQL-Server-connections:\" data-toc-modified-id=\"SQL-Server-connections:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>SQL Server connections:</a></span></li></ul></li><li><span><a href=\"#Functions-to-get-tweets,-clean-it,-analyze,-stock-price-and-update-into-the-SQL-server\" data-toc-modified-id=\"Functions-to-get-tweets,-clean-it,-analyze,-stock-price-and-update-into-the-SQL-server-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions to get tweets, clean it, analyze, stock price and update into the SQL server</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-data-from-Twitter:\" data-toc-modified-id=\"Get-data-from-Twitter:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Get data from Twitter:</a></span><ul class=\"toc-item\"><li><span><a href=\"#API-Connection-to-the-selected-type-of-search:\" data-toc-modified-id=\"API-Connection-to-the-selected-type-of-search:-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>API Connection to the selected type of search:</a></span></li><li><span><a href=\"#Get-tweets-data-with-Tweepy\" data-toc-modified-id=\"Get-tweets-data-with-Tweepy-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Get tweets data with Tweepy</a></span></li><li><span><a href=\"#Tweets-Table\" data-toc-modified-id=\"Tweets-Table-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Tweets Table</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global-function\" data-toc-modified-id=\"Global-function-2.1.3.1\"><span class=\"toc-item-num\">2.1.3.1&nbsp;&nbsp;</span>Global function</a></span></li><li><span><a href=\"#Tweets-elements-function:\" data-toc-modified-id=\"Tweets-elements-function:-2.1.3.2\"><span class=\"toc-item-num\">2.1.3.2&nbsp;&nbsp;</span>Tweets elements function:</a></span></li><li><span><a href=\"#Geocode-data-function:\" data-toc-modified-id=\"Geocode-data-function:-2.1.3.3\"><span class=\"toc-item-num\">2.1.3.3&nbsp;&nbsp;</span>Geocode data function:</a></span></li><li><span><a href=\"#URL-Title-function:\" data-toc-modified-id=\"URL-Title-function:-2.1.3.4\"><span class=\"toc-item-num\">2.1.3.4&nbsp;&nbsp;</span>URL Title function:</a></span></li><li><span><a href=\"#Translate-Tweets-that-are-not-in-english:\" data-toc-modified-id=\"Translate-Tweets-that-are-not-in-english:-2.1.3.5\"><span class=\"toc-item-num\">2.1.3.5&nbsp;&nbsp;</span>Translate Tweets that are not in english:</a></span></li></ul></li><li><span><a href=\"#Hashtag-Table:\" data-toc-modified-id=\"Hashtag-Table:-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Hashtag Table:</a></span></li></ul></li><li><span><a href=\"#Text-cleaning:\" data-toc-modified-id=\"Text-cleaning:-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Text cleaning:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transform-to-lower:\" data-toc-modified-id=\"Transform-to-lower:-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Transform to lower:</a></span></li><li><span><a href=\"#Remove-URLS\" data-toc-modified-id=\"Remove-URLS-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Remove URLS</a></span></li><li><span><a href=\"#Emoticons-&amp;-Emojis:\" data-toc-modified-id=\"Emoticons-&amp;-Emojis:-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Emoticons &amp; Emojis:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Emoticons\" data-toc-modified-id=\"Emoticons-2.2.3.1\"><span class=\"toc-item-num\">2.2.3.1&nbsp;&nbsp;</span>Emoticons</a></span></li><li><span><a href=\"#Emojis\" data-toc-modified-id=\"Emojis-2.2.3.2\"><span class=\"toc-item-num\">2.2.3.2&nbsp;&nbsp;</span>Emojis</a></span></li></ul></li><li><span><a href=\"#Remove-punctuations,-hashtag-and-mentions:\" data-toc-modified-id=\"Remove-punctuations,-hashtag-and-mentions:-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Remove punctuations, hashtag and mentions:</a></span></li><li><span><a href=\"#Remove-keywords-that-we-are-using-for-the-search:\" data-toc-modified-id=\"Remove-keywords-that-we-are-using-for-the-search:-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Remove keywords that we are using for the search:</a></span></li><li><span><a href=\"#Remove-stop-words:\" data-toc-modified-id=\"Remove-stop-words:-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Remove stop words:</a></span></li><li><span><a href=\"#Remove-non-alphabetic-words:\" data-toc-modified-id=\"Remove-non-alphabetic-words:-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>Remove non alphabetic words:</a></span></li><li><span><a href=\"#Remove-tweets-that-are-empty-after-cleaning\" data-toc-modified-id=\"Remove-tweets-that-are-empty-after-cleaning-2.2.8\"><span class=\"toc-item-num\">2.2.8&nbsp;&nbsp;</span>Remove tweets that are empty after cleaning</a></span></li></ul></li><li><span><a href=\"#Text-Mining-Analysis:\" data-toc-modified-id=\"Text-Mining-Analysis:-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Text Mining Analysis:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Monogram-analysis:\" data-toc-modified-id=\"Monogram-analysis:-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Monogram analysis:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize-words:\" data-toc-modified-id=\"Tokenize-words:-2.3.1.1\"><span class=\"toc-item-num\">2.3.1.1&nbsp;&nbsp;</span>Tokenize words:</a></span></li><li><span><a href=\"#Fix-word-lengthening:\" data-toc-modified-id=\"Fix-word-lengthening:-2.3.1.2\"><span class=\"toc-item-num\">2.3.1.2&nbsp;&nbsp;</span>Fix word lengthening:</a></span></li><li><span><a href=\"#TextBlob-sentiment-analysis:\" data-toc-modified-id=\"TextBlob-sentiment-analysis:-2.3.1.3\"><span class=\"toc-item-num\">2.3.1.3&nbsp;&nbsp;</span>TextBlob sentiment analysis:</a></span></li><li><span><a href=\"#Afinn-sentiment-analysis:\" data-toc-modified-id=\"Afinn-sentiment-analysis:-2.3.1.4\"><span class=\"toc-item-num\">2.3.1.4&nbsp;&nbsp;</span>Afinn sentiment analysis:</a></span></li></ul></li><li><span><a href=\"#Bigram-analysis:\" data-toc-modified-id=\"Bigram-analysis:-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Bigram analysis:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenize-bigrams:\" data-toc-modified-id=\"Tokenize-bigrams:-2.3.2.1\"><span class=\"toc-item-num\">2.3.2.1&nbsp;&nbsp;</span>Tokenize bigrams:</a></span></li><li><span><a href=\"#Afinn-sentiment-analysis:\" data-toc-modified-id=\"Afinn-sentiment-analysis:-2.3.2.2\"><span class=\"toc-item-num\">2.3.2.2&nbsp;&nbsp;</span>Afinn sentiment analysis:</a></span></li></ul></li><li><span><a href=\"#Sentence-analysis:\" data-toc-modified-id=\"Sentence-analysis:-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Sentence analysis:</a></span><ul class=\"toc-item\"><li><span><a href=\"#TextBlob-sentiment:\" data-toc-modified-id=\"TextBlob-sentiment:-2.3.3.1\"><span class=\"toc-item-num\">2.3.3.1&nbsp;&nbsp;</span>TextBlob sentiment:</a></span></li><li><span><a href=\"#Afinn-analysis:\" data-toc-modified-id=\"Afinn-analysis:-2.3.3.2\"><span class=\"toc-item-num\">2.3.3.2&nbsp;&nbsp;</span>Afinn analysis:</a></span></li></ul></li><li><span><a href=\"#URL-Title-analysis\" data-toc-modified-id=\"URL-Title-analysis-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>URL Title analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Afinn-analysis:\" data-toc-modified-id=\"Afinn-analysis:-2.3.4.1\"><span class=\"toc-item-num\">2.3.4.1&nbsp;&nbsp;</span>Afinn analysis:</a></span></li></ul></li></ul></li><li><span><a href=\"#Get-Stock-Price-data:\" data-toc-modified-id=\"Get-Stock-Price-data:-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Get Stock Price data:</a></span></li><li><span><a href=\"#Update-into-the-SQL-Server\" data-toc-modified-id=\"Update-into-the-SQL-Server-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Update into the SQL Server</a></span><ul class=\"toc-item\"><li><span><a href=\"#Connect-to-the-SQL-Server:\" data-toc-modified-id=\"Connect-to-the-SQL-Server:-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Connect to the SQL Server:</a></span></li><li><span><a href=\"#Update-tables:\" data-toc-modified-id=\"Update-tables:-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Update tables:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tweets_Table:\" data-toc-modified-id=\"Tweets_Table:-2.5.2.1\"><span class=\"toc-item-num\">2.5.2.1&nbsp;&nbsp;</span>Tweets_Table:</a></span></li><li><span><a href=\"#Hashtags-table:\" data-toc-modified-id=\"Hashtags-table:-2.5.2.2\"><span class=\"toc-item-num\">2.5.2.2&nbsp;&nbsp;</span>Hashtags table:</a></span></li><li><span><a href=\"#Monogram-table:\" data-toc-modified-id=\"Monogram-table:-2.5.2.3\"><span class=\"toc-item-num\">2.5.2.3&nbsp;&nbsp;</span>Monogram table:</a></span></li><li><span><a href=\"#Bigram-table:\" data-toc-modified-id=\"Bigram-table:-2.5.2.4\"><span class=\"toc-item-num\">2.5.2.4&nbsp;&nbsp;</span>Bigram table:</a></span></li><li><span><a href=\"#StockPrice-table:\" data-toc-modified-id=\"StockPrice-table:-2.5.2.5\"><span class=\"toc-item-num\">2.5.2.5&nbsp;&nbsp;</span>StockPrice table:</a></span></li></ul></li><li><span><a href=\"#Close-SQL-Connection\" data-toc-modified-id=\"Close-SQL-Connection-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Close SQL Connection</a></span></li></ul></li><li><span><a href=\"#Global-functions:\" data-toc-modified-id=\"Global-functions:-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Global functions:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Twitter-connection-regarding-type-of-search-and-data-collection:\" data-toc-modified-id=\"Twitter-connection-regarding-type-of-search-and-data-collection:-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>Twitter connection regarding type of search and data collection:</a></span></li><li><span><a href=\"#Text-cleaning:\" data-toc-modified-id=\"Text-cleaning:-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>Text cleaning:</a></span></li><li><span><a href=\"#Text-mining-analysis:\" data-toc-modified-id=\"Text-mining-analysis:-2.6.3\"><span class=\"toc-item-num\">2.6.3&nbsp;&nbsp;</span>Text mining analysis:</a></span></li><li><span><a href=\"#Update-into-SQL-Server\" data-toc-modified-id=\"Update-into-SQL-Server-2.6.4\"><span class=\"toc-item-num\">2.6.4&nbsp;&nbsp;</span>Update into SQL Server</a></span></li><li><span><a href=\"#Run-everything:\" data-toc-modified-id=\"Run-everything:-2.6.5\"><span class=\"toc-item-num\">2.6.5&nbsp;&nbsp;</span>Run everything:</a></span></li></ul></li></ul></li><li><span><a href=\"#Run-all:\" data-toc-modified-id=\"Run-all:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Run all:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.202928Z",
     "start_time": "2020-08-06T10:30:18.418369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spavo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import itertools\n",
    "import collections\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "import schedule\n",
    "import time\n",
    "import pyodbc\n",
    "import schedule\n",
    "import time\n",
    "import yaml\n",
    "from searchtweets import collect_results, ResultStream, gen_rule_payload, load_credentials\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import textblob\n",
    "import seaborn as sns\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from afinn import Afinn\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import html\n",
    "from termcolor import colored\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import pytz\n",
    "import warnings\n",
    "from googletrans import Translator\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Parameters Definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Tweepy connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.213931Z",
     "start_time": "2020-08-06T10:30:24.204921Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#3 types of search: Normal(daily tweets) / 30days (Premium search last 30 days tweets) /\n",
    "# Full_Archive (Premium Search betweent the date specified)\n",
    "\n",
    "Type_of_Search = 'Normal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Search parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.226862Z",
     "start_time": "2020-08-06T10:30:24.215893Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Company = 'TSLA'\n",
    "search_words = \"#Tesla\"\n",
    "nb_items = 10\n",
    "keywords = ['Tesla', 'ElonMusk']\n",
    "lang = 'all'\n",
    "#Start date of the search (for normal search it will be the current day)\n",
    "# date_since = str(date.today())\n",
    "date_since = '2020-07-02'\n",
    "# #End date (only for full archive search)\n",
    "to_date = '2020-07-29'\n",
    "Res_per_call = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## SQL Server connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.234841Z",
     "start_time": "2020-08-06T10:30:24.229857Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Driver_Type = '{SQL Server}'\n",
    "Server_Name = 'SQL_SERVER_NAME'\n",
    "Database_Name = 'TABLE'\n",
    "Trusted_Connection_Type = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.242821Z",
     "start_time": "2020-08-06T10:30:24.237834Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Driver = 'Driver=' + Driver_Type + ';'\n",
    "Server = 'Server=' + Server_Name + ';'\n",
    "Database = 'Database=' + Database_Name + ';'\n",
    "Trusted_connection = 'Trusted_Connection=' + Trusted_Connection_Type + ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.249802Z",
     "start_time": "2020-08-06T10:30:24.245812Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SQL_ID = Driver + Server + Database + Trusted_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to get tweets, clean it, analyze, stock price and update into the SQL server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from Twitter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### API Connection to the selected type of search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.267754Z",
     "start_time": "2020-08-06T10:30:24.252794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Tweepy_Type_Connection(Type_of_Search):\n",
    "#If the type of search is normal, we use Tweepy package\n",
    "    if Type_of_Search == \"Normal\":\n",
    "        from ID import Tokens\n",
    "        # Load Twitter API credentials from the Py file\n",
    "        api_key = Tokens['Key']\n",
    "        api_secret_key = Tokens['Secret_Key']\n",
    "        access_token = Tokens['Token']\n",
    "        access_token_secret = Tokens['Secret_Token']\n",
    "        # Connect to Twitter API using the credentials\n",
    "        auth = tw.OAuthHandler(api_key, api_secret_key)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        api = tw.API(auth)\n",
    "        \n",
    "        return api\n",
    "    #For premium search, we use searchtweets\n",
    "    elif Type_of_Search == \"30days\":\n",
    "        premium_search_args_30days = load_credentials(\"D:/OneDrive/Documents/Local_Github/Text Mining Twitter with Python and Power BI Dashboard/Code/twitter_keys_30Days.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api_30Days\",\n",
    "                                       env_overwrite=False)\n",
    "        \n",
    "        return premium_search_args_30days\n",
    "    \n",
    "    elif Type_of_Search == \"Full_Archive\":\n",
    "        premium_search_args_fullarchive = load_credentials(\"D:/OneDrive/Documents/Local_Github/Text Mining Twitter with Python and Power BI Dashboard/Code/twitter_keys_fullarchive.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "        return premium_search_args_fullarchive\n",
    "    \n",
    "    else:\n",
    "        print(\"Please type a correct type of search\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.281716Z",
     "start_time": "2020-08-06T10:30:24.274735Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "api = Tweepy_Type_Connection(Type_of_Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get tweets data with Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.301664Z",
     "start_time": "2020-08-06T10:30:24.285706Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_tweet(search_words, date_since, nb_items):\n",
    "#If the language isn't english, we don't put filter on language   \n",
    "    if lang != 'en':\n",
    "        lang_search = ''\n",
    "    else:\n",
    "        lang_search = 'lang:en'\n",
    "    \n",
    "    if Type_of_Search == \"Normal\":\n",
    "        \n",
    "        if lang == 'en':\n",
    "       #We get the tweet with tweepy \n",
    "            cursorTweet = tw.Cursor(api.search,\n",
    "                       q=search_words + ' -filter:retweets',\n",
    "                       lang=\"en\",\n",
    "                       since=date_since).items(nb_items)\n",
    "        else:\n",
    "            \n",
    "            cursorTweet = tw.Cursor(api.search,\n",
    "                       q=search_words + ' -filter:retweets',\n",
    "                       since=date_since).items(nb_items)\n",
    "    \n",
    "    #We get the tweet with searchtweets for premium search\n",
    "    elif Type_of_Search == \"Full_Archive\":\n",
    "        \n",
    "        rule_fullarchive = gen_rule_payload(search_words + ' ' + lang_search , \n",
    "                                            results_per_call= Res_per_call, \n",
    "                                            from_date= date_since, \n",
    "                                            to_date= to_date)\n",
    "        \n",
    "        cursorTweet = collect_results(rule_fullarchive, \n",
    "                         max_results=nb_items, \n",
    "                         result_stream_args=api)\n",
    "    \n",
    "    elif Type_of_Search == \"30days\":\n",
    "        \n",
    "        rule_30Days = gen_rule_payload(search_words + ' ' + lang_search , results_per_call=Res_per_call, \n",
    "                                       from_date = date_since,\n",
    "                                       to_date= to_date)\n",
    "        \n",
    "        cursorTweet = collect_results(rule_30Days, \n",
    "                         max_results=nb_items, \n",
    "                         result_stream_args=api)\n",
    "\n",
    "    #We append the data in a list\n",
    "    tweets = []\n",
    "\n",
    "    for item in cursorTweet:\n",
    "        if Type_of_Search == \"Normal\":\n",
    "            tweets.append(item)\n",
    "        \n",
    "        else:\n",
    "\n",
    "            if item.text.startswith(\"RT @\") == False:\n",
    "                tweets.append(item)\n",
    "        \n",
    "    print(\"Tweppy has sucessfully collected the Twitter data\")\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Global function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.312634Z",
     "start_time": "2020-08-06T10:30:24.304655Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Initate a function to run all bellow functions\n",
    "def get_elements_Tweet_Table(tweets):\n",
    "    print(\"Elements for the Tweets table will be collected\")\n",
    "    data = select_elements(tweets)\n",
    "    data = get_geocode(tweets,data)\n",
    "    data = get_url_title(tweets, data)\n",
    "    data = translate_tweet(data)\n",
    "    print('Elements for the Tweets table have been sucessfully collected')\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Tweets elements function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.330586Z",
     "start_time": "2020-08-06T10:30:24.315626Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def select_elements(tweets):\n",
    "    #Filter the tweets if we do a normal search\n",
    "    if Type_of_Search == \"Normal\":\n",
    "\n",
    "        data = [[tweet.id, tweet.created_at, tweet.text,\n",
    "                 tweet.lang, tweet.user.location, tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=['ID', 'Date', 'Text', 'Language', 'Location', 'Retweet', 'Favorite'])\n",
    "    \n",
    "    #Filter the tweets if we do a premium search\n",
    "    else:\n",
    "        \n",
    "        data = [[tweet.id, tweet['created_at'], tweet.text,\n",
    "                 tweet.lang, tweet['user']['location'], tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "        data = pd.DataFrame(\n",
    "            data, columns=['ID', 'Date', 'Text', 'Language', 'Location', 'Retweet', 'Favorite'])\n",
    "    \n",
    "    #Transform the date to +0.\n",
    "    if Type_of_Search != \"Normal\":\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        data['Date'] = data['Date'].dt.tz_localize(None)\n",
    "    \n",
    "    print('Keys elements have been collected')\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Geocode data function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.347541Z",
     "start_time": "2020-08-06T10:30:24.333578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Initiate a function to get geocode information\n",
    "def get_geocode(tweets, data):\n",
    "\n",
    "    geocode = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        if Type_of_Search == \"Normal\":\n",
    "            if tweet.coordinates is None:\n",
    "                geocode.append(tweet.coordinates)\n",
    "            else:\n",
    "                geocode.append(tweet.coordinates[\"coordinates\"])\n",
    "        else:\n",
    "            if tweet['coordinates'] is None:\n",
    "                geocode.append(tweet['coordinates'])\n",
    "            else:\n",
    "                geocode.append(tweet['coordinates'][\"coordinates\"])\n",
    "\n",
    "    latitude=[]\n",
    "    longitude=[]\n",
    "\n",
    "    for i in geocode:\n",
    "        if i is not None:\n",
    "            latitude.append(i[0])\n",
    "            longitude.append(i[1])\n",
    "        else:\n",
    "            latitude.append(i)\n",
    "            longitude.append(i)\n",
    "\n",
    "    data['Latitude']=latitude\n",
    "    data['Longitude']=longitude\n",
    "\n",
    "    data['Latitude']=data['Latitude'].astype(str)\n",
    "    data['Longitude']=data['Longitude'].astype(str)\n",
    "\n",
    "    print(\"Geolocalisation data points have been collected\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### URL Title function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.372473Z",
     "start_time": "2020-08-06T10:30:24.350532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We use beautifulsoup to get url titles scrapping the h1 of each url found in the tweets\n",
    "def get_url_title(tweets, data):\n",
    "\n",
    "    url_title = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        if Type_of_Search == \"Normal\":\n",
    "            if len(tweet.entities['urls']) == 0:\n",
    "                url_title.append(None)\n",
    "            else:\n",
    "                url = tweet.entities['urls'][0]['expanded_url']\n",
    "\n",
    "                try:\n",
    "                    html_content = requests.get(url).text\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    requests.status_code = \"Connection refused\"\n",
    "\n",
    "                soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "                try:\n",
    "                    url_title.append(soup.title.text)\n",
    "                except:\n",
    "                    url_title.append(None)\n",
    "        else:\n",
    "            if len(tweet['entities']['urls']) == 0:\n",
    "                url_title.append(None)\n",
    "            else:\n",
    "                url = tweet['entities']['urls'][0]['expanded_url']\n",
    "\n",
    "                try:\n",
    "                    html_content = requests.get(url).text\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    requests.status_code = \"Connection refused\"\n",
    "\n",
    "                soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "                try:\n",
    "                    url_title.append(soup.title.text)\n",
    "                except:\n",
    "                    url_title.append(None)\n",
    "\n",
    "    data['URL_Title'] = url_title\n",
    "    \n",
    "    for i in range(0, len(data['URL_Title'])):\n",
    "        if data['URL_Title'][i] != None:\n",
    "            if data['URL_Title'][i] in '403 Forbidden':\n",
    "                data['URL_Title'][i] = None\n",
    "                   \n",
    "\n",
    "    print('URL Titles have been collected')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate Tweets that are not in english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.390437Z",
     "start_time": "2020-08-06T10:30:24.375466Z"
    }
   },
   "outputs": [],
   "source": [
    "#For non english tweets, we translate them into english using googletrans package\n",
    "def translate_tweet(data):\n",
    "    \n",
    "    if lang != 'en':\n",
    "    \n",
    "        translator = Translator()\n",
    "    \n",
    "        for i in range(0, len(data)):\n",
    "            if data['Language'][i] != 'en':\n",
    "                translate = translator.translate(data['Text'][i])\n",
    "                data['Text'][i] = translate.text\n",
    "            \n",
    "            if data['URL_Title'][i] != None:\n",
    "                if translator.translate(data['URL_Title'][i]).src != 'en':\n",
    "                    translate = translator.translate(data['URL_Title'][i])\n",
    "                    data['URL_Title'][i] = translate.text\n",
    "                    \n",
    "    print('Tweets and URLS have been translated to english')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Hashtag Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.413364Z",
     "start_time": "2020-08-06T10:30:24.394415Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We extract the hashtags from the data we got in twitter\n",
    "def get_hashtag(tweets):\n",
    "\n",
    "    hashtags = []\n",
    "    hash_in_tweets = []\n",
    "    Tweet_ID = []\n",
    "    for tweet in tweets:\n",
    "        if Type_of_Search == \"Normal\":\n",
    "            \n",
    "            if tweet.entities['hashtags'] is None:\n",
    "                hashtags.append(None)\n",
    "            else:\n",
    "                for i in range(0, len(tweet.entities['hashtags'])):\n",
    "                    hash_in_tweets.append(tweet.entities[\"hashtags\"][i]['text'])\n",
    "                hashtags.append(hash_in_tweets)\n",
    "                Tweet_ID.append(tweet.id)\n",
    "                hash_in_tweets = []\n",
    "        else:\n",
    "            if tweet['entities']['hashtags'] is None:\n",
    "                hashtags.append(None)\n",
    "            else:\n",
    "                for i in range(0, len(tweet['entities']['hashtags'])):\n",
    "                    hash_in_tweets.append(tweet['entities'][\"hashtags\"][i]['text'])\n",
    "                hashtags.append(hash_in_tweets)\n",
    "                Tweet_ID.append(tweet.id)\n",
    "                hash_in_tweets = []\n",
    "\n",
    "    Hashtags_data = pd.DataFrame(Tweet_ID, columns = ['Tweet_ID'])\n",
    "    Hashtags_data['Hashtags'] = hashtags\n",
    "    \n",
    "    hashtag_list = []\n",
    "    ID_Hash = []\n",
    "    Counter = 0\n",
    "\n",
    "    for i in Hashtags_data['Hashtags']:\n",
    "        for hashtag in i:\n",
    "            hashtag_list.append(hashtag)\n",
    "            ID_Hash.append(Hashtags_data['Tweet_ID'][Counter])\n",
    "        Counter = Counter + 1\n",
    "\n",
    "    Hashtags_Final = pd.DataFrame(hashtag_list, columns = ['Hashtag'])\n",
    "    Hashtags_Final['Tweet_ID'] = ID_Hash\n",
    "\n",
    "    print(\"Hashtags have been collected\")\n",
    "    \n",
    "    print('Elements for the Hashtags table have been successfully collected')\n",
    "    \n",
    "    return Hashtags_Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Text cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Transform to lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.422340Z",
     "start_time": "2020-08-06T10:30:24.416357Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def to_lower(data, Text):\n",
    "    \n",
    "    data['Text_Cleaned'] = data[Text]\n",
    "    \n",
    "    data['Text_Cleaned'] = data['Text_Cleaned'].str.lower()\n",
    "    \n",
    "    print('Text has been transformed to lowercase')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.430326Z",
     "start_time": "2020-08-06T10:30:24.424334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_urls(data):\n",
    "    \n",
    "    data['Text_Cleaned'] = data['Text_Cleaned'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    \n",
    "    print(\"URLs have been removed\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Emoticons & Emojis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.446276Z",
     "start_time": "2020-08-06T10:30:24.432313Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We initiate lists of emoticons and count their occurence by type \n",
    "def emoticons_count(data):\n",
    "\n",
    "    #Happy emoticons\n",
    "    emoticons_happy = set([\n",
    "        ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "        ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "        '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "        'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "        '<3'\n",
    "        ])\n",
    "\n",
    "    #Sad Emoticons\n",
    "    emoticons_sad = set([\n",
    "        ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "        ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "        ':c', ':{', '>:\\\\', ';('\n",
    "        ])\n",
    "\n",
    "    count_happy = 0\n",
    "    count_sad = 0\n",
    "    happy = []\n",
    "    sad = []\n",
    "\n",
    "    for tweet in data['Text']:\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            if word in emoticons_happy:\n",
    "                count_happy = count_happy + 1\n",
    "            elif word in emoticons_sad:\n",
    "                count_sad = count_sad + 1\n",
    "        happy.append(count_happy)\n",
    "        sad.append(count_sad)\n",
    "        count_happy = 0\n",
    "        count_sad  = 0\n",
    "        \n",
    "    data['Emoticons_Happy'] = count_happy\n",
    "    data['Emoticons_Sad'] =  count_sad\n",
    "    \n",
    "    print('Emoticons have been analyzed')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.459250Z",
     "start_time": "2020-08-06T10:30:24.448270Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We initiate lists of emojis and count their occurence by type \n",
    "\n",
    "def emojis_count(data):\n",
    "    \n",
    "    emoji_happy = [':smiley:', ':kissing_closed_eyes:', ':purple_heart:', ':heartpulse:', ':+1:', ':clap:', ':smile:', \\\n",
    "              ':heart:', ':two_hearts:', ':thumbsup:', ':fire:', ':muscle:', ':heart_eyes_cat:', ':simple_smile:', ':smirk:', \\\n",
    "              ':relieved:', ':green_heart:', ':revolving_hearts:', ':open_hands:', ':raised_hands:', ':heart_eyes:', \\\n",
    "              ':yum:', ':yellow_heart:', ':cupid:', ':v:', ':kissing_heart:', ':kissing:', ':heartbeat:', ':sparkling_heart:', \\\n",
    "              ':ok_hand:', ':smiley_cat:']\n",
    "    \n",
    "    emoji_sad = [':anguished:', ':weary:', ':rage:', ':crying_cat_face:', ':unamused:', ':pensive:', ':persevere:', ':anger:', \\\n",
    "            ':disappointed:', ':cry:', ':sleepy:', ':imp:', ':-1:', ':pouting_cat:', ':worried:', ':sweat:', ':confounded:', \\\n",
    "            ':sob:', ':sob:', ':broken_heart:', ':poop:', ':thumbsdown:', ':fu:', ':frowning:', ':disappointed_relieved:', \\\n",
    "            ':fearful:', ':angry:', ':shit:', ':negative_squared_cross_mark:', ':x:']\n",
    "    \n",
    "    count_happy_emoji = 0\n",
    "    happy_emojis = []\n",
    "    count_sad_emoji= 0\n",
    "    sad_emojis = []\n",
    "\n",
    "    for tweet in data['Text']:\n",
    "\n",
    "        words = tweet.split()\n",
    "\n",
    "        for word in words:\n",
    "\n",
    "            for happy_emo in emoji_happy:\n",
    "\n",
    "                if word in emoji.emojize(happy_emo, use_aliases=True):\n",
    "\n",
    "                    count_happy_emoji = count_happy_emoji + 1\n",
    "\n",
    "            for sad_emo in emoji_sad:\n",
    "\n",
    "                if word in emoji.emojize(sad_emo, use_aliases=True):\n",
    "\n",
    "                    count_sad_emoji = count_sad_emoji + 1\n",
    "\n",
    "        sad_emojis.append(count_sad_emoji)\n",
    "        count_sad_emoji = 0\n",
    "        happy_emojis.append(count_happy_emoji)\n",
    "        count_happy_emoji = 0\n",
    "        \n",
    "    data['Emojis_Happy'] = happy_emojis\n",
    "    data['Emojis_Sad'] =  sad_emojis\n",
    "    \n",
    "    print('Emojis have been analyzed')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T09:28:39.187582Z",
     "start_time": "2020-07-29T09:28:39.182599Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove punctuations, hashtag and mentions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.469217Z",
     "start_time": "2020-08-06T10:30:24.462245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_special_character(data):\n",
    "    \n",
    "    ToRemove = ['[^\\w\\s]']\n",
    "    \n",
    "    for i in ToRemove:\n",
    "        data['Text_Cleaned'] = data['Text_Cleaned'].str.replace(r'[^\\w\\s]', '', case=False)\n",
    "        \n",
    "    print(\"Punctuations and special characters have been removed\")\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove keywords that we are using for the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.478190Z",
     "start_time": "2020-08-06T10:30:24.471212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_keywords(data, keywords):\n",
    "    \n",
    "    for i in keywords:\n",
    "        data['Text_Cleaned'] = data['Text_Cleaned'].str.replace(i, '', case=False)\n",
    "        \n",
    "    print(\"Keywords have been removed\")\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T09:32:40.845342Z",
     "start_time": "2020-07-29T09:32:40.817419Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.488165Z",
     "start_time": "2020-08-06T10:30:24.480185Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    data['Text_Cleaned'] = data['Text_Cleaned'].apply(lambda x: ' '.join([word for word in x.split() \\\n",
    "                                                                          if word not in (stop_words)]))\n",
    "    print('Stop words have bee removed')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove non alphabetic words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.501130Z",
     "start_time": "2020-08-06T10:30:24.496145Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_non_alpha(data):\n",
    "    \n",
    "    data['Text_Cleaned'] = data['Text_Cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "    \n",
    "    print('Non alphabetic words have been removed')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T09:36:05.540938Z",
     "start_time": "2020-07-29T09:36:05.512977Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Remove tweets that are empty after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.511104Z",
     "start_time": "2020-08-06T10:30:24.505118Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_empty(data):\n",
    "    \n",
    "    data = data[data.Text_Cleaned != '']\n",
    "    \n",
    "    data = data.reset_index()\n",
    "    \n",
    "    print(\"Empty tweets have been removed\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Text Mining Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Monogram analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Tokenize words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.522073Z",
     "start_time": "2020-08-06T10:30:24.514094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We tokenize the words into monogram\n",
    "def tokenize_words(data, ID, text):\n",
    "    counter = 0\n",
    "    Token_Word = []\n",
    "    ID_Word = []\n",
    "    IDcol = data[ID]\n",
    "    \n",
    "    for i in data[text]:\n",
    "        x = i\n",
    "        for word in i.split():\n",
    "            Token_Word.append(word)\n",
    "            ID_Word.append(IDcol[counter])\n",
    "        counter = counter + 1\n",
    "\n",
    "\n",
    "            \n",
    "    Words_Mono = pd.DataFrame()\n",
    "    Words_Mono['Word'] = Token_Word\n",
    "    Words_Mono['Word_ID'] = ID_Word\n",
    "\n",
    "    return Words_Mono"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Fix word lengthening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.531050Z",
     "start_time": "2020-08-06T10:30:24.524067Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fix_lengthening(words): \n",
    "    def reduce_lengthening(text):\n",
    "        pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "        return pattern.sub(r\"\\1\\1\", text)\n",
    "    \n",
    "    for index, word in enumerate(words['Word']):\n",
    "        words['Word'][index] =  reduce_lengthening(word)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T09:45:06.001064Z",
     "start_time": "2020-07-29T09:45:05.989066Z"
    },
    "hidden": true
   },
   "source": [
    "#### TextBlob sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.540024Z",
     "start_time": "2020-08-06T10:30:24.534042Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Sentiment analysis of monograms with TextBlob\n",
    "def sentiment_TextBlob(Words_Tokenized):\n",
    "    analysis_TextBlob = []\n",
    "    for i in Words_Tokenized['Word']:\n",
    "        sentiment = TextBlob(i)\n",
    "        analysis_TextBlob.append(sentiment.polarity)\n",
    "\n",
    "    Words_Tokenized['Sentiment_TextBlob'] =  analysis_TextBlob\n",
    "    \n",
    "    return Words_Tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Afinn sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.548004Z",
     "start_time": "2020-08-06T10:30:24.542020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Sentiment analysis of monograms with Afinn dictionnary\n",
    "def sentiment_Afinn(Words_Tokenized):\n",
    "    afinn = Afinn()\n",
    "\n",
    "    analysis_afinn = []\n",
    "    for i in Words_Tokenized['Word']:\n",
    "        sentiment = afinn.score(i)\n",
    "        analysis_afinn.append(sentiment)\n",
    "\n",
    "    Words_Tokenized['Sentiment_Afinn'] = analysis_afinn\n",
    "\n",
    "    return Words_Tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Bigram analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Tokenize bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.558974Z",
     "start_time": "2020-08-06T10:30:24.550995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Tokenize the words into bigrams\n",
    "def tokenize_bigrams(data, ID, text):\n",
    "    counter = 0\n",
    "    Bigram_Word = []\n",
    "    ID_Word = []\n",
    "    IDcol = data[ID]\n",
    "    bigrams = []\n",
    "    \n",
    "    for i in data[text]:\n",
    "        nltk_tokens = nltk.word_tokenize(i)\n",
    "        bigrams.append((list(nltk.bigrams(nltk_tokens))))\n",
    "        \n",
    "    \n",
    "    for i in bigrams:\n",
    "        for bigram in i:\n",
    "            Bigram_Word.append(bigram)\n",
    "            ID_Word.append(IDcol[counter])\n",
    "        counter = counter + 1\n",
    "        \n",
    "    Words_Bigram = pd.DataFrame()\n",
    "    Words_Bigram['Bigram'] = Bigram_Word    \n",
    "    Words_Bigram = pd.DataFrame(Words_Bigram['Bigram'].to_list(), columns = ['Word1','Word2'])\n",
    "    Words_Bigram['Bigram_ID'] = ID_Word\n",
    "        \n",
    "    return Words_Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Afinn sentiment analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.569945Z",
     "start_time": "2020-08-06T10:30:24.562967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Sentimental analysis of bigrams with Afinn dictionnary\n",
    "def bigram_Afinn(Words_Bigram):\n",
    "    \n",
    "    analysis_afinn = []\n",
    "\n",
    "    afinn = Afinn()\n",
    "\n",
    "    for i in Words_Bigram['Word1']:\n",
    "        sentiment = afinn.score(i)\n",
    "        analysis_afinn.append(sentiment)\n",
    "\n",
    "    Words_Bigram['Word1_Afinn'] = analysis_afinn\n",
    "    \n",
    "    analysis_afinn = []\n",
    "\n",
    "    afinn = Afinn()\n",
    "\n",
    "    for i in Words_Bigram['Word2']:\n",
    "        sentiment = afinn.score(i)\n",
    "        analysis_afinn.append(sentiment)\n",
    "\n",
    "    Words_Bigram['Word2_Afinn'] = analysis_afinn\n",
    "    \n",
    "    return Words_Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Sentence analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### TextBlob sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.578921Z",
     "start_time": "2020-08-06T10:30:24.571940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Sentimental analysis of full tweets with TextBlob\n",
    "def tweet_sentiment_TextBlob(data):\n",
    "    \n",
    "    analysis_TextBlob = []\n",
    "    \n",
    "    for i in data['Text_Cleaned']:\n",
    "        sentiment = TextBlob(i)\n",
    "        analysis_TextBlob.append(sentiment.polarity)\n",
    "    \n",
    "    data['Tweet_Sent_TextBlob'] =  analysis_TextBlob\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Afinn analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.587897Z",
     "start_time": "2020-08-06T10:30:24.580916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Sentimental analysis of full tweets with Afinn dictionnary\n",
    "def tweet_sentiment_Afinn(data):\n",
    "    \n",
    "    afinn = Afinn()\n",
    "    \n",
    "    analysis_Afinn = []\n",
    "    \n",
    "    for i in data['Text_Cleaned']:\n",
    "        sentiment = afinn.score(i)\n",
    "        analysis_Afinn.append(sentiment)\n",
    "    \n",
    "    data['Tweet_Sent_Afinn'] =  analysis_Afinn\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T10:02:47.112843Z",
     "start_time": "2020-07-29T10:02:47.073946Z"
    },
    "hidden": true
   },
   "source": [
    "### URL Title analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Afinn analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.599864Z",
     "start_time": "2020-08-06T10:30:24.589893Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Sentimental analysis of full tweets with Afinn dictionnary\n",
    "def url_title_sent_Afinn(data):\n",
    "    afinn = Afinn()\n",
    "\n",
    "    URL_Sentiment = []\n",
    "\n",
    "    for i in data['URL_Title']:\n",
    "        if i is None:\n",
    "            sentiment = 0\n",
    "        else:\n",
    "            sentiment = afinn.score(i)\n",
    "        URL_Sentiment.append(sentiment)\n",
    "\n",
    "    data['URL_Sentiment'] = URL_Sentiment\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Get Stock Price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.610836Z",
     "start_time": "2020-08-06T10:30:24.601860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get the daily stockprice value of Tesla actions\n",
    "def get_Stock_Price(Company):\n",
    "    \n",
    "    start = str(date.today() - datetime.timedelta(days = 1)) \n",
    "    close = str(date.today() - datetime.timedelta(days = 0))\n",
    "\n",
    "    try:\n",
    "        StockPrice = yf.download(tickers= Company, interval=\"60m\", start = start, end = close)\n",
    "        StockPrice.index = StockPrice.index.tz_convert(\"UTC\")\n",
    "        StockPrice = StockPrice.reset_index()\n",
    "        StockPrice = StockPrice[['Datetime', 'Open']]\n",
    "        StockPrice['Datetime'] = StockPrice['Datetime'].dt.tz_localize(None)\n",
    "        \n",
    "    except:\n",
    "        print('This company is not listed on the stock exchange.')\n",
    "        \n",
    "    \n",
    "    print('StockPrice of ' + Company + ' are collected and up to date')\n",
    "    return StockPrice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Update into the SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Connect to the SQL Server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.618815Z",
     "start_time": "2020-08-06T10:30:24.612830Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Connection to our local SQL Server\n",
    "def SQL_connection(SQL_ID):\n",
    "    \n",
    "    conn = pyodbc.connect(SQL_ID)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Update tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Tweets_Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.633774Z",
     "start_time": "2020-08-06T10:30:24.620809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Update the data in our SQL Server and remove potential duplicates\n",
    "def update_tweets_data(tweet_table, conn, cursor):\n",
    "    \n",
    "    for index, row in tweet_table.iterrows():\n",
    "        cursor.execute(\"INSERT INTO Tesla_Tweets([ID], [Date_Time],[Tweet],[Language],[Location],[Latitude],[Longitude], \\\n",
    "            [Retweet], [Favorite], [URL_Title], [Text_Cleaned], [Emoticons_Happy], [Emoticons_Sad], [Emojis_Happy], \\\n",
    "            [Emojis_Sad], [Tweet_Sent_TextBlob], [Tweet_Sent_Afinn], [URL_Sentiment]) \\\n",
    "        values(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\",\n",
    "                       row['ID'], \n",
    "                       row['Date'], \n",
    "                       row['Text'], \n",
    "                       row['Language'], \n",
    "                       row['Location'], \n",
    "                       row['Latitude'],\n",
    "                       row['Longitude'],\n",
    "                       row['Retweet'],\n",
    "                       row['Favorite'],\n",
    "                       row['URL_Title'],\n",
    "                       row['Text_Cleaned'],\n",
    "                       row['Emoticons_Happy'],\n",
    "                       row['Emoticons_Sad'],\n",
    "                       row['Emojis_Happy'],\n",
    "                       row['Emojis_Sad'],\n",
    "                       row['Tweet_Sent_TextBlob'],\n",
    "                       row['Tweet_Sent_Afinn'],\n",
    "                       row['URL_Sentiment'])\n",
    "\n",
    "    conn.commit()\n",
    "    \n",
    "    cursor.execute(\"WITH cte AS ( \\\n",
    "        SELECT \\\n",
    "            ID, \\\n",
    "            Date_Time, \\\n",
    "            ROW_NUMBER() OVER ( \\\n",
    "                PARTITION BY \\\n",
    "                    ID, \\\n",
    "                    Date_Time \\\n",
    "                ORDER BY \\\n",
    "                    ID, \\\n",
    "                    Date_Time \\\n",
    "            ) row_num \\\n",
    "         FROM \\\n",
    "            Tesla_Tweets \\\n",
    "        ) \\\n",
    "        DELETE FROM cte \\\n",
    "        WHERE row_num > 1\")\n",
    "\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"Tweets table has been updated with the new data\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Hashtags table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.645741Z",
     "start_time": "2020-08-06T10:30:24.636767Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Update the data in our SQL Server and remove potential duplicates\n",
    "def update_hashtags_table(hashtags_table, conn, cursor):\n",
    "\n",
    "    for index, row in hashtags_table.iterrows():\n",
    "        cursor.execute(\"INSERT INTO Hashtags([Tweet_ID], [Hashtag]) \\\n",
    "        values(?,?)\",\n",
    "                       row['Tweet_ID'], \n",
    "                       row['Hashtag'])\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.execute(\"WITH cte AS ( \\\n",
    "        SELECT \\\n",
    "            Tweet_ID, \\\n",
    "            Hashtag, \\\n",
    "            ROW_NUMBER() OVER ( \\\n",
    "                PARTITION BY \\\n",
    "                    Tweet_ID, \\\n",
    "                    Hashtag \\\n",
    "                ORDER BY \\\n",
    "                    Tweet_ID, \\\n",
    "                    Hashtag \\\n",
    "            ) row_num \\\n",
    "         FROM \\\n",
    "            Hashtags \\\n",
    "        ) \\\n",
    "        DELETE FROM cte \\\n",
    "        WHERE row_num > 1\")\n",
    "\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"Hashtags table has been updated with the new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Monogram table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.655716Z",
     "start_time": "2020-08-06T10:30:24.648735Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Update the data in our SQL Server and remove potential duplicates\n",
    "def update_monogram_table(monogram_table, conn, cursor):\n",
    "\n",
    "    for index, row in monogram_table.iterrows():\n",
    "            cursor.execute(\"INSERT INTO Monogram([Word], [Word_ID], [Sent_Blob], [Sent_Afinn]) \\\n",
    "            values(?,?,?,?)\",\n",
    "                           row['Word'], \n",
    "                           row['Word_ID'], \n",
    "                           row['Sentiment_TextBlob'], \n",
    "                           row['Sentiment_Afinn'])\n",
    "\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Monogram table has been updated with the new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Bigram table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.664692Z",
     "start_time": "2020-08-06T10:30:24.658707Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Update the data in our SQL Server and remove potential duplicates\n",
    "def update_bigram_table(bigram_table, conn, cursor):\n",
    "    \n",
    "    for index, row in bigram_table.iterrows():\n",
    "        cursor.execute(\"INSERT INTO Bigram([Word1], [Word2], [Word1_Afinn], [Word2_Afinn], [Bigram_ID]) \\\n",
    "        values(?,?,?,?,?)\",\n",
    "                       row['Word1'], \n",
    "                       row['Word2'], \n",
    "                       row['Word1_Afinn'],\n",
    "                       row['Word2_Afinn'],\n",
    "                       row['Bigram_ID'])\n",
    "\n",
    "    cursor.execute(\"WITH cte AS ( \\\n",
    "            SELECT \\\n",
    "                Word1, \\\n",
    "                Word2,\\\n",
    "                Bigram_ID,\\\n",
    "                ROW_NUMBER() OVER ( \\\n",
    "                    PARTITION BY \\\n",
    "                        Word1, \\\n",
    "                        Word2, \\\n",
    "                        Bigram_ID \\\n",
    "                    ORDER BY \\\n",
    "                        Word1, \\\n",
    "                        Word2, \\\n",
    "                        Bigram_ID \\\n",
    "                ) row_num \\\n",
    "             FROM \\\n",
    "                Bigram \\\n",
    "            ) \\\n",
    "            DELETE FROM cte \\\n",
    "            WHERE row_num > 1\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Bigram table has been updated with the new data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### StockPrice table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.673668Z",
     "start_time": "2020-08-06T10:30:24.667685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Update the data in our SQL Server and remove potential duplicates\n",
    "def update_stockprice_table(stockprice_table, conn, cursor):\n",
    "    \n",
    "    for index, row in stockprice_table.iterrows():\n",
    "        cursor.execute(\"INSERT INTO StockPrice([Datetime], [Value_Open]) \\\n",
    "        values(?,?)\",\n",
    "                       row['Datetime'], \n",
    "                       row['Open'])\n",
    "\n",
    "    cursor.execute(\"WITH cte AS ( \\\n",
    "            SELECT \\\n",
    "                Datetime, \\\n",
    "                ROW_NUMBER() OVER ( \\\n",
    "                    PARTITION BY \\\n",
    "                        Datetime \\\n",
    "                    ORDER BY \\\n",
    "                        Datetime \\\n",
    "                ) row_num \\\n",
    "             FROM \\\n",
    "                StockPrice \\\n",
    "            ) \\\n",
    "            DELETE FROM cte \\\n",
    "            WHERE row_num > 1\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"StockPrice table has been updated with the new data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Close SQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.680649Z",
     "start_time": "2020-08-06T10:30:24.675662Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def close_SQL_connection(cursor):\n",
    "    return cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Global functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Twitter connection regarding type of search and data collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.690622Z",
     "start_time": "2020-08-06T10:30:24.683641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_data_twitter(search_words, date_since, nb_items):\n",
    "\n",
    "    tweets = get_tweet(search_words, date_since, nb_items)\n",
    "    data = get_elements_Tweet_Table(tweets)\n",
    "    hashtags = get_hashtag(tweets)\n",
    "    \n",
    "    return data, hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Text cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.700594Z",
     "start_time": "2020-08-06T10:30:24.692617Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_tweet(data, Text, keywords):\n",
    "    data = to_lower(data, Text)\n",
    "    data = remove_urls(data)\n",
    "    data = emoticons_count(data)\n",
    "    data = emojis_count(data)\n",
    "    data = remove_special_character(data)\n",
    "    data = remove_keywords(data, keywords)\n",
    "    data = remove_stop_words(data)\n",
    "    data = remove_non_alpha(data)\n",
    "    data = remove_empty(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Text mining analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.710569Z",
     "start_time": "2020-08-06T10:30:24.702590Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def text_mining_analysis(data, ID, text):\n",
    "    Words_Tokenized = tokenize_words(data, ID, text)\n",
    "    Words_Tokenized = fix_lengthening(Words_Tokenized)\n",
    "    Words_Tokenized = sentiment_TextBlob(Words_Tokenized)\n",
    "    Words_Tokenized = sentiment_Afinn(Words_Tokenized)\n",
    "    \n",
    "    print('Words have been tokenized to monogram and analyzed')\n",
    "    \n",
    "    Words_Bigram = tokenize_bigrams(data, ID, text)\n",
    "    Words_Bigram = bigram_Afinn(Words_Bigram)\n",
    "    \n",
    "    print('Words have been tokenized to bigram and analyzed')\n",
    "    \n",
    "    data = tweet_sentiment_TextBlob(data)\n",
    "    data = tweet_sentiment_Afinn(data)\n",
    "    data = url_title_sent_Afinn(data)\n",
    "    \n",
    "    print('URLs and tweets have been analyzed')\n",
    "    \n",
    "    return Words_Tokenized, Words_Bigram, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Update into SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.720542Z",
     "start_time": "2020-08-06T10:30:24.712563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def update_SQL_Server(SQL_ID, tweet_table, hashtags_table, monogram_table, bigram_table,stockprice_table):\n",
    "    \n",
    "    conn, cursor = SQL_connection(SQL_ID)\n",
    "    update_tweets_data(tweet_table, conn, cursor)\n",
    "    update_hashtags_table(hashtags_table, conn, cursor)\n",
    "    update_monogram_table(monogram_table, conn, cursor)\n",
    "    update_bigram_table(bigram_table, conn, cursor)\n",
    "    update_stockprice_table(stockprice_table, conn, cursor)\n",
    "    close_SQL_connection(cursor)\n",
    "    print(\"All tables have been updated and the connection is now closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Run everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:24.734509Z",
     "start_time": "2020-08-06T10:30:24.723534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    \n",
    "    print(colored(\"Step 1: Data collection\", 'green', attrs=['bold']))\n",
    "    data, hashtags = get_data_twitter(search_words, date_since, nb_items)\n",
    "    print(\"\")\n",
    "    \n",
    "    print(colored(\"Step 2: Data cleaning\", 'green', attrs=['bold']))\n",
    "    data = clean_tweet(data, 'Text', keywords)\n",
    "    print(\"\")\n",
    "\n",
    "    print(colored(\"Step 3: Text mining analysis\", 'green', attrs=['bold']))\n",
    "    Words_Tokenized, Words_Bigram, data = text_mining_analysis(data, 'ID', 'Text_Cleaned')\n",
    "    print(\"\")\n",
    "    \n",
    "    print(colored(\"Step 4: Getting stock price data\", 'green', attrs = ['bold']))\n",
    "    StockPrice = get_Stock_Price(Company)\n",
    "    print('')\n",
    "        \n",
    "    print(colored(\"Step 5: Update of the data into the SQL Server\", 'green', attrs=['bold']))\n",
    "    update_SQL_Server(SQL_ID, data, hashtags, Words_Tokenized, Words_Bigram, StockPrice)\n",
    "\n",
    "    print('')\n",
    "    print(colored(\"Data has been collected, cleaned and updated without problems\", 'green', attrs=['bold']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T10:30:43.112855Z",
     "start_time": "2020-08-06T10:30:24.736499Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32mStep 1: Data collection\u001b[0m\n",
      "Tweppy has sucessfully collected the Twitter data\n",
      "Elements for the Tweets table will be collected\n",
      "Keys elements have been collected\n",
      "Geolocalisation data points have been collected\n",
      "URL Titles have been collected\n",
      "Tweets and URLS have been translated to english\n",
      "Elements for the Tweets table have been sucessfully collected\n",
      "(10, 10)\n",
      "Hashtags have been collected\n",
      "Elements for the Hashtags table have been successfully collected\n",
      "\n",
      "\u001b[1m\u001b[32mStep 2: Data cleaning\u001b[0m\n",
      "Text has been transformed to lowercase\n",
      "URLs have been removed\n",
      "Emoticons have been analyzed\n",
      "Emojis have been analyzed\n",
      "Punctuations and special characters have been removed\n",
      "Keywords have been removed\n",
      "Stop words have bee removed\n",
      "Non alphabetic words have been removed\n",
      "Empty tweets have been removed\n",
      "\n",
      "\u001b[1m\u001b[32mStep 3: Text mining analysis\u001b[0m\n",
      "Words have been tokenized to monogram and analyzed\n",
      "Words have been tokenized to bigram and analyzed\n",
      "URLs and tweets have been analyzed\n",
      "\n",
      "\u001b[1m\u001b[32mStep 4: Getting stock price data\u001b[0m\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- TSLA: No data found for this date range, symbol may be delisted\n",
      "This company is not listed on the stock exchange.\n",
      "StockPrice of TSLA are collected and up to date\n",
      "\n",
      "\u001b[1m\u001b[32mStep 5: Update of the data into the SQL Server\u001b[0m\n",
      "Tweets table has been updated with the new data\n",
      "Hashtags table has been updated with the new data\n",
      "Monogram table has been updated with the new data\n",
      "Bigram table has been updated with the new data\n",
      "StockPrice table has been updated with the new data\n",
      "All tables have been updated and the connection is now closed\n",
      "\n",
      "\u001b[1m\u001b[32mData has been collected, cleaned and updated without problems\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run_all()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
